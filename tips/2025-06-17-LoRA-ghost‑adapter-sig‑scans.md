# ğŸ¤–ğŸ”’ AI-Sec Tip â€” 2025-06-17

## LoRA â€œghostâ€‘adapterâ€ sigâ€‘scans: 
Before you merge or hotâ€‘swap any Lowâ€‘Rank Adaptation (LoRA) file, run a deterministic *digest pass* over its matrices and compare against a registry of approved hashes.
Why? The *LoRAâ€‘asâ€‘anâ€‘Attack* study shows a 4â€¯MB adapter can hide policyâ€‘evasion triggers and backâ€‘doors that survive fullâ€‘precision merges.
Followâ€‘up work such as *TeleLoRA* demonstrates how an attacker can teleport a single, permutationâ€‘symmetric backâ€‘door across model families, dodging baseâ€‘model checksums.
Meanwhile, recent analyses of LLM backâ€‘door longevity confirm that tokenâ€‘level triggers planted in adapters can persist for thousands of context tokensâ€”long enough to fire inside downstream RAG (Retrievalâ€‘Augmented Generation) chains.

Dropâ€‘in sentinel (PyTorch, 6 lines):

```python
import hashlib, torch, pathlib

def lora_digest(path):
    st = torch.load(path, map_location="cpu")
    h = hashlib.sha256()
    for k in sorted([k for k in st if k.endswith("lora_A")]):
        h.update(st[k].cpu().numpy().tobytes())
    return h.hexdigest()[:16]          # 64â€‘bit fingerprint
```

1. **On boot**: crawl your `./adapters/` folder, compute `lora_digest()`, and write the fingerprints to an allowâ€‘list YAML committed in CI.
2. **At runtime**: any API call that requests an adapter first reâ€‘hashes and *fails closed* if the fingerprint is unknown.
3. **Bonus trick**: sprinkle the same hash into the adapterâ€™s filename (e.g., `qa_policy_9f3a1b2c.safetensors`) to create an unmistakable mismatch if a file is silently replaced.

**Outcome**: One SHAâ€‘256 pass (<2â€¯ms per adapter) turns the â€œshareâ€‘andâ€‘playâ€ attack surface into a simple hashâ€‘match gateâ€”cheap insurance against ghostly LoRA hijacks.

![img](../assets/2025-06-17-LoRA-ghostâ€‘adapter-sigâ€‘scan.png)


| Panel                       | Flow                                                                                                                                                   | Key Take-aways                                                                                                 |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------- |
| **LoRA-as-an-Attack**       | Attacker âœ *malicious LoRA* (.safetensors, \~4 MB) âœ Public hub âœ Dev merges into base LLM âœ Hidden trigger in user prompt fires.                      | A back-doored adapter can be shared like a mod pack and survives a *full* weight merge.                        |
| **TeleLoRA**                | *Permutation-symmetric* LoRA generator spits out one adapter âœ Works across Model A (7 B), Model B (13 B), even vision-LLMs âœ Same trigger, same harm. | TeleLoRA â€œteleportsâ€ alignment (or mis-alignment) into *any* architecture, sidestepping model-specific checks. |


### Defense block (bottom of each panel)

*Compute `lora_digest()` âœ compare against an allow-list YAML âœ **reject unknown fingerprints**.*

This <2 ms hash gate turns the LoRA supply chain into a whitelisted clubâ€”no extra GPUs, no fancy audits.

### Why it matters

* **Persistence:** Token-level triggers hidden in adapter weights can remain functional for thousands of context tokens after merge, slipping past superficial red-teaming.
* **Portability:** TeleLoRA proves that â€œone-size-fits-allâ€ backdoors are feasibleâ€”even across size, architecture, and modality.
* **Supply-chain risk:** Community-shared LoRA hubs multiply exposure; OWASPâ€™s 2025 LLM Top 10 now lists *LoRA Adapter Tampering* under â€œLLM-03: Supply Chain.â€

  

**Pro-tip:** Store the truncated 64-bit digest in the adapterâ€™s filename (`chat_guard_9f3a1b2c.safetensors`). Any silent swap shows up as an instant mismatch on disk *and* at load-time.

**Resources that used for creatng this bite size tip:**

\[1]: ["LoRA-as-an-Attack! Piercing LLM Safety Under the Share-and-Play Scenario"](https://arxiv.org/abs/2403.00108)

\[2]: ["LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem"](https://openreview.net/forum?id=0owyEm6FAk)

\[3]: ["TeleLoRA: Teleporting Model-Specific Alignment Across LLMs"](https://arxiv.org/abs/2503.20228)

\[4]: ["LLM03:2025 Supply Chain â€“ OWASP Gen AI Security Project"](https://genai.owasp.org/llmrisk/llm032025-supply-chain/)

\[5]: ["OWASP Top 10 for LLM Applications 2025 (PDF)"](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-v2025.pdf)

\[6]: ["Hey, Thatâ€™s My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique"](https://arxiv.org/html/2407.10887v3)

\[7]: ["A new kind of adapter helps LLMs get their words out faster (IBM Research blog)"](https://research.ibm.com/blog/inference-friendly-aloras-lora)

\[8]: ["BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge"](https://arxiv.org/abs/2503.00596)

\[9]: ["SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain"](https://arxiv.org/abs/2502.12497)

\[10]: ["The Philosopherâ€™s Stone: Trojaning Plugins of Large Language Models"](https://arxiv.org/abs/2312.00374)

\[11]: ["Weight Poisoning Attacks on Pre-trained Models"](https://arxiv.org/abs/2004.06660)

â€” End â€”
